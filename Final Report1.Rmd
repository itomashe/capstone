---
title: "Final Notes"

date: "Friday, April 22, 2016"
output: html_document
---

The report contains the final notes for the Capstone project.  

###Data Transformations:
Data Transformations described in the Milestone report are limited to a small percentage of the available data due to limitation of R and available hardware. Alternative way would be to use scripts shown in [1]. We applied the below sripts just to 'news' data set. 
It could be easily expanded to other data sets. List of R libraries is below:
```{r warning=FALSE}
library(qdap)
library(tm)
library(stringr)
library(SnowballC)
library(RWeka)
library(ggplot2)
library(swiftcap)
library(shiny)

news.data<-scan(what="c",sep="\n",file="en_US.news.txt")

````
In order to built and test model, we need to split a data set into train (~95%) and test (~5%) :
```{r}
samplen<-length(news.data)
news.data.train<-news.data[1:floor(samplen/1.05)]
news.data.test<-news.data[(length(news.data.train)+1):samplen]
news.data<-news.data.train     ## not needed, included just to combine pieces of code from different sources.
news.data<-tolower(news.data)
tokens<-unlist(strsplit(news.data,"[^a-z]+"))
tokens<-tokens[tokens!=""]
n=length(tokens)
unigram<-sort(table(tokens),decreasing=T)
tokens2<-c(tokens[-1],".")
freq<-sort(table(paste(tokens,tokens2)),decreasing=T)     #bigram frequency
tokens3<-c(tokens2[1],".")
trigrams<-paste(tokens,tokens2,tokens3)
trigram_freq<-sort(table(trigrams),decreasing=T)
unigram.df<-data.frame(unigram=names(unigram),freq=unigram)
bigram.df<-data.frame(bigram=names(freq),freq=freq)
trigram.df<-data.frame(trigram=names(trigram_freq),freq=trigram_freq)
prob<-bigram.df$freq/n
bigram.df<-cbind(bigram.df,prob)
prob<-trigram.df$freq/n
trigram.df<-cbind(trigram.df,prob)

````
We have data frames for unigrams, bigrams and trigrams. At this point we have enough data to calculate the maximum likelihood estimate of the probability of N-gram based on the news data.
For example:  
P('jerry brown')=count('jerry brown')/count('jerry')

The major problem with MLE is that it assigns zero probabilty to any N-gram not in the Corpus(news data set)
To avoid this problem 'smoothing' is used, to assign a small but non-zero probability to these "zero probability N-grams"
Back-off is another method for dealing with unseen n-gram.
Katz's back-off model and news data set will be used to create the Shiny application.
The estimate for n-gram is allowed to back off through shorter histories.
If n-gram has appered more than k times(k is set to 0), then an n-fram estimate is used, if n-gram did not appear then we will use an estimate from a shorter n-gram. This recursion can continue down, so that we can start with a trigram model and end up estimating the next word based on unigram frequencies.
Such models are simple and in practice work well. Examples and details of such implementation can be found [2,3,4,5,6 and 7]


### References

- [1] Bigrams and Trigrams. John Fry. www.english.boisestate.edu/johnfry/files/2013/04/bigram-2x2.pdf
- [2] Foundations of Statistical Natural Language Processing. Christopher D. Manning and Hinrich Schutze. MIT, 1999
- [3] Hands -On Data Science with R Text Mining. Graham Williams. www.handsondatascience.com/TextMiningO.pdf
- [4] DNatural Language Processing: A model to Predict a sequence of Words. Gerald R Gendron, Jr. www.modsimworld.org/papers/2015/Natural_Language_Processing.pdf.
- [5] An R package for creating N-gram language models. https://github/nickwallen/swiftcap
- [6] Speech and Language Processing: An Introduction to natural language processing. Daniel Jurafsky & James Martin. 200
- [7] An empirical study of smoothing techniques for lanuage modeling. Stanley F. Chen and Joshua Goodman. Computer Speech and Language (1999) 13.

  